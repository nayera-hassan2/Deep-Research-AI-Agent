import os
import json
import logging
import requests
from typing import TypedDict
from dotenv import load_dotenv
from langchain_community.tools import TavilySearchResults
from langgraph.graph import StateGraph
from transformers import pipeline


# ===========================
# ğŸ”¹ CONFIGURATION
# ===========================
# Setting up API keys 
load_dotenv()  # Loading API keys from .env file

# Fetching API keys securely
TAVILY_API_KEY = os.getenv("TAVILY_API_KEY", "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")

# Ensuring API key is present
if not TAVILY_API_KEY:
    raise ValueError("API key missing! Ensure .env file contains TAVILY_API_KEY.")

# Seting up API key in environment
os.environ["TAVILY_API_KEY"] = "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"


# Initializing Logger/ log file
logging.basicConfig(filename="ai_research.log", level=logging.INFO, format="%(asctime)s - %(message)s")


# Initializing Summarization Model Globally
try:
    summarizer = pipeline("summarization", model="facebook/bart-large-cnn")  # Load BART once
    logging.info("âœ… Summarization model loaded successfully.")
except Exception as e:
    logging.error(f"âš ï¸ Error loading summarization model: {e}")
    summarizer = None  # Prevent crashes

# =========================== 
# ğŸ”¹ STATE SCHEMA
# ===========================
class ResearchState(TypedDict):
    query: str
    data: str
    summary: str


# ===========================
# ğŸ”¹ RESEARCH AGENT (Web Crawler)
# ===========================
def fetch_research_data(state: ResearchState) -> ResearchState:
    """Fetches research data using Tavily Search API."""
    query = state["query"]

    try:
        search_tool = TavilySearchResults()
        results = search_tool.invoke({"query": query, "num_results": 5})  # Fetch top 5 search results

        if not results:
            logging.warning(f"âš ï¸ No relevant data found for query: {query}")
            return {"query": query, "data": "No relevant data found.", "summary": ""}

        logging.info(f"âœ… Fetched {len(results)} search results for query: {query}")
        return {"query": query, "data": json.dumps(results, indent=2), "summary": ""}

    except Exception as e:
        logging.error(f"âŒ Research Agent Error: {e}")
        return {"query": query, "data": "Error retrieving research data.", "summary": ""}


# ===========================
# ğŸ”¹ Local DRAFTING AGENT (Summarizes Data) (No API Needed)
# ===========================
def generate_summary(state: ResearchState) -> ResearchState:
    """Summarizes research findings using a local Transformer model (offline)."""
    data = state["data"]

    if not data or data == "No relevant data found.":
        return {"query": state["query"], "data": data, "summary": "âš ï¸ No data available for summarization."}

    if summarizer is None:
        return {"query": state["query"], "data": data, "summary": "âš ï¸ Summarization model is unavailable."}

    try:
        summary = summarizer(data[:1024], max_length=150, min_length=50, do_sample=False)[0]["summary_text"]
        logging.info("âœ… Successfully generated summary using local model.")
        return {"query": state["query"], "data": data, "summary": summary}

    except Exception as e:
        logging.error(f"âŒ Summarization Agent Error: {e}")
        return {"query": state["query"], "data": data, "summary": "âš ï¸ Error generating summary."}


# ===========================
# ğŸ”¹ BUILD THE GRAPH (Fix for SimpleSequentialChain)
# ===========================
graph = StateGraph(ResearchState)
graph.add_node("ResearchAgent", fetch_research_data)
graph.add_node("SummarizationAgent", generate_summary)

graph.add_edge("ResearchAgent", "SummarizationAgent")  # Linking nodes
graph.set_entry_point("ResearchAgent")  # Setting entry point

workflow = graph.compile()  # Compiling workflow


# ===========================
# ğŸ”¹ RUN THE SYSTEM (Pipeline)
# ===========================
def run_ai_research_system(query: str) -> str:
    """Runs the AI research pipeline locally and saves output."""
    logging.info(f"ğŸ” Processing query: {query}")
    result = workflow.invoke({"query": query, "data": "", "summary": ""})
    
    # Save output to file
    output_data = {"query": query, "summary": result["summary"]}
    with open("research_output.json", "w") as file:
        json.dump(output_data, file, indent=4)
    
    return result["summary"]


def run_interactive_mode():
    """Interactive mode for user input."""
    query = input("ğŸ” Enter a research topic: ").strip()

    if not query:
        print("âŒ Error: Query cannot be empty.")
        return

    print("\nğŸš€ Researching... Please wait...")
    research_data = fetch_research_data({"query": query, "data": "", "summary": ""})["data"]

    if research_data and research_data != "No relevant data found.":
        print("\nğŸ“ Drafting Summary...")
        summary = generate_summary({"query": query, "data": research_data, "summary": ""})["summary"]

        # Save output to file
        output_data = {"query": query, "summary": summary}
        with open("research_output.json", "w") as file:
            json.dump(output_data, file, indent=4)

        print("\nâœ… Research Summary:\n", summary)
    else:
        print("\nâš ï¸ No research data found!")


# ===========================
# ğŸ”¹ MAIN EXECUTION
# ===========================
if __name__ == "__main__":
    query = "Latest advancements in AI research"
    output = run_ai_research_system(query)
    print("\nğŸ“„ Final Research Summary:\n", output)